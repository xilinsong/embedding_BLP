{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLP extension with product embeddings\n",
    "\n",
    "code adapted from [wangzhegeek](https://github.com/wangzhegeek/EGES/tree/master), using Nevo (2000) fake cereal data from [PyBLP](https://pyblp.readthedocs.io/en/stable/index.html) library.\n",
    "> Nevo, A. (2000), A Practitioner's Guide to Estimation of Random-Coefficients Logit Models of Demand. Journal of Economics & Management Strategy, 9: 513-548."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment: python 3.6.x, tensorflow==1.14.0\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import networkx as nx\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some tools first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alias sampling\n",
    "def create_alias_table(area_ratio):\n",
    "    '''\n",
    "    area_ratio: list, elements add up to 1, the probability distribution to sample from\n",
    "    '''\n",
    "    l = len(area_ratio)\n",
    "    area_ratio = [prop*l for prop in area_ratio]\n",
    "    accept, alias = [0]*l, [0]*l\n",
    "    small, large = [], []\n",
    "    for i, prob in enumerate(area_ratio):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "            \n",
    "    while small and large:\n",
    "        small_index, large_index = small.pop(), large.pop()\n",
    "        accept[small_index] = area_ratio[small_index]\n",
    "        alias[small_index] = large_index\n",
    "        area_ratio[large_index] = area_ratio[large_index] - (1-area_ratio[small_index])\n",
    "        if area_ratio[large_index] < 1.0:\n",
    "            small.append(large_index)\n",
    "        else:\n",
    "            large.append(large_index)\n",
    "    while large:\n",
    "        large_index = large.pop()\n",
    "        accept[large_index] = 1\n",
    "    while small:\n",
    "        small_index = small.pop()\n",
    "        accept[small_index] = 1\n",
    "    return accept, alias\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random()*N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_num(num, workers):\n",
    "    if num % workers == 0:\n",
    "        return [num//workers]*workers\n",
    "    else:\n",
    "        return [num//workers]*workers + [num%workers]\n",
    "    \n",
    "def graph_context_batch_iter(all_pairs, batch_size, side_info, num_attr):\n",
    "    while True:\n",
    "        start_idx = np.random.randint(0, len(all_pairs)-batch_size)\n",
    "        batch_idx = np.array(range(start_idx, start_idx+batch_size))\n",
    "        batch_idx = np.random.permutation(batch_idx)\n",
    "        batch = np.zeros((batch_size, num_attr),dtype=np.int32)\n",
    "        labels = np.zeros((batch_size,1),dtype=np.int32)\n",
    "        batch[:] = side_info[all_pairs[batch_idx,0]]\n",
    "        labels[:,0] = all_pairs[batch_idx,1]\n",
    "        yield batch, labels\n",
    "        \n",
    "def write_embedding(embedding_result, output_filename):\n",
    "    f = open(output_filename,'w')\n",
    "    for i in range(len(embedding_result)):\n",
    "        s = \",\".join(str(f) for f in embedding_result[i].tolist())\n",
    "        f.write(s + '\\n')\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "####### TODO: Adapt this #######\n",
    "def plot_embedding(embed_mat, side_info_mat):\n",
    "    model = TSNE(n_components=2)\n",
    "    node_pos = model.fit_transform(embed_mat)\n",
    "    # suppose there are 3 product attributes\n",
    "    attr1_idx, attr2_idx, attr3_idx = {}, {}, {}\n",
    "    for i in range(len(node_pos)):\n",
    "        attr1_idx.setdefault(side_info_mat[i,1],[])\n",
    "        attr1_idx[side_info_mat[i,1]].append(i)\n",
    "        attr2_idx.setdefault(side_info_mat[i,2],[])\n",
    "        attr2_idx[side_info_mat[i,2]].append(i)\n",
    "        attr3_idx.setdefault(side_info_mat[i,3],[])\n",
    "        attr3_idx[side_info_mat[i,3]].append(i)\n",
    "    \n",
    "    plt.figure()\n",
    "    for c, idx in attr1_idx.items():\n",
    "        plt.scatter(node_pos[idx,0], node_pos[idx,1], label=c)\n",
    "    plt.title('Attribute 1 distribution')\n",
    "    plt.savefig(\"./nevodata_cache/attr1.png\")\n",
    "    \n",
    "    plt.figure()\n",
    "    for c, idx in attr2_idx.items():\n",
    "        plt.scatter(node_pos[idx,0], node_pos[idx,1], label=c)\n",
    "    plt.title('Attribute 2 distribution')\n",
    "    plt.savefig(\"./nevodata_cache/attr2.png\")\n",
    "\n",
    "    plt.figure()\n",
    "    for c, idx in attr3_idx.items():\n",
    "        plt.scatter(node_pos[idx,0], node_pos[idx,1], label=c)\n",
    "    plt.title('Attribute 3 distribution')\n",
    "    plt.savefig(\"./nevodata_cache/attr3.png\")\n",
    "    return\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalker:\n",
    "    \n",
    "    # deepwalk performs first order random walk\n",
    "    # node2vec performs second order random walk, which incorporates information from the previous node.\n",
    "    \n",
    "    def __init__(self,G,p=1,q=1):\n",
    "        \"\"\"\n",
    "        G: the graph\n",
    "        p: return parameter. p controls the likelihood of immediately returning to a node we just visited.\n",
    "        q: in-out parameter. q controls how likely we are to stay in the neighborhood of a certain node, \n",
    "                                or are we more likely to visit nodes further away from this node.\n",
    "        (when p=q=1, it becomes first order random walk/1-hop transition)\n",
    "        G.neighbors(k): Returns an iterator over all neighbors of node k\n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        \n",
    "    def deepwalk_walk(self,walk_length,start_node):\n",
    "        \n",
    "        walk = [start_node]\n",
    "        \n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1] # current node\n",
    "            cur_nbrs = list(self.G.neighbors(cur)) # current node neighbors\n",
    "            if len(cur_nbrs) > 0:\n",
    "                walk.append(random.choice(cur_nbrs))\n",
    "            else:\n",
    "                break\n",
    "        return walk\n",
    "    \n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "        G = self.G\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "        \n",
    "        walk = [start_node]\n",
    "        \n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = list(self.G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                if len(walk) == 1:\n",
    "                    walk.append(cur_nbrs[alias_sample(alias_nodes[cur][0],alias_nodes[cur][1])])\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    edge = (prev, cur)\n",
    "                    next_node = cur_nbrs[alias_sample(alias_edges[edge][0],alias_edges[edge][1])]\n",
    "                    walk.append(next_node)\n",
    "            else:\n",
    "                break\n",
    "        return walk\n",
    "    \n",
    "    def simulate_walks(self, num_walks, walk_length, workers=1, verbose=0):\n",
    "        G = self.G\n",
    "        nodes = list(G.nodes())\n",
    "        results = Parallel(n_jobs=workers,verbose=verbose,)(delayed(self._simulate_walks)(nodes, num, walk_length) for num in partition_num(num_walks,workers))\n",
    "        walks = list(itertools.chain(*results))\n",
    "        return walks\n",
    "    \n",
    "    def _simulate_walks(self, nodes, num_walks, walk_length,):\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for v in nodes:\n",
    "                if self.p == 1 and self.q == 1:\n",
    "                    walks.append(self.deepwalk_walk(walk_length=walk_length, start_node=v))\n",
    "                else:\n",
    "                    walks.append(self.node2vec_walk(walk_length=walk_length, start_node=v))\n",
    "        return walks\n",
    "    \n",
    "    def get_alias_edge(self, t, v):\n",
    "        '''\n",
    "        get the alias table for node2vec_walk\n",
    "        '''\n",
    "        G = self.G\n",
    "        p = self.p\n",
    "        q = self.q\n",
    "        \n",
    "        unnormalized_probs = []\n",
    "        for x in G.neighbors(v):\n",
    "            weight = G[v][x].get('weight',1.0)\n",
    "            if x == t:\n",
    "                unnormalized_probs.append(weight/p)\n",
    "            elif G.has_edge(x,t):\n",
    "                unnormalized_probs.append(weight)\n",
    "            else:\n",
    "                unnormalized_probs.append(weight/q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs = [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "        \n",
    "        alias_edge = create_alias_table(normalized_probs)\n",
    "        return alias_edge\n",
    "    \n",
    "    def preprocess_transition_probs(self):\n",
    "        \n",
    "        G =  self.G\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr].get('weight',1.0) for nbr in G.neighbors(node)]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs = [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = create_alias_table(normalized_probs)\n",
    "        \n",
    "        alias_edges = {}\n",
    "        for edge in G.edges():\n",
    "            alias_edges[edge] = self.get_alias_edge(edge[0],edge[1])\n",
    "        \n",
    "        self.alias_nodes = alias_nodes\n",
    "        self.alias_edges = alias_edges\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nevo_data():\n",
    "    '''\n",
    "    recover user history from agent_data:\n",
    "        (1) recover each consumer's choice in each market\n",
    "        (2) assume consumers with the same {income, age, child} are the same people, give them a unique user_id\n",
    "    '''\n",
    "    \n",
    "    # recover choices\n",
    "    product_data = pd.read_csv(\"./nevodata/product_data.csv\")\n",
    "    agent_data = pd.read_csv(\"./nevodata/agent_data.csv\")\n",
    "\n",
    "    product = product_data[['market_ids', 'product_ids', 'prices', 'sugar', 'mushy']].copy()\n",
    "    product.insert(2, 'constant', np.ones(product.shape[0]))\n",
    "    agent = agent_data[['market_ids','nodes0','nodes1','nodes2','nodes3','income','income_squared','age','child']].copy()\n",
    "\n",
    "    # variable order: constant, price, sugar, mushy; income, income_squared, age, child \n",
    "    beta = np.array([-1.841, -32.433, 0.148, 0.788])\n",
    "    sigma = np.diag([0.377, 1.848, 0.004, 0.081])\n",
    "    pi = np.array([\n",
    "      [ 3.089,  0,      1.186,  0     ],\n",
    "      [16.598, -0.659,  0,      11.625],\n",
    "      [-0.193,  0,      0.029,  0     ],\n",
    "      [ 1.468,  0,     -1.514,  0     ]\n",
    "    ])\n",
    "    \n",
    "    market_list = product['market_ids'].unique()\n",
    "    for market in market_list:\n",
    "        sub_agent = agent[agent['market_ids']==market]\n",
    "        sub_product = product[product['market_ids']==market]\n",
    "        choice = []\n",
    "        for i in range(sub_agent.shape[0]):\n",
    "            v_list = []\n",
    "            for j in range(sub_product.shape[0]):\n",
    "                mu = np.dot((np.dot(sigma,sub_agent.iloc[i,1:5]) + np.dot(pi,sub_agent.iloc[i,5:9])),sub_product.iloc[j,2:6])\n",
    "                delta = np.dot(beta,sub_product.iloc[j,2:6])\n",
    "                v_list.append(mu+delta)\n",
    "            choice_idx = (np.exp(v_list)/np.sum(np.exp(v_list))).argmax()\n",
    "            choice_product = sub_product.iloc[choice_idx,1]\n",
    "            choice.append(choice_product)\n",
    "        agent.loc[agent['market_ids']==market,'product_ids'] = choice\n",
    "        \n",
    "    \n",
    "    # assign user_id\n",
    "    agent['user_id'] = agent.apply(lambda row: hash((row['age'], row['child'], row['income'])), axis=1)\n",
    "    agent['user_id'] = agent['user_id'].abs()\n",
    "    action_data = agent[['user_id','product_ids']].copy()\n",
    "    \n",
    "    return action_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(action_data, use_type=None):\n",
    "    '''\n",
    "    action_data: agent history recordings\n",
    "    returns a list of sessions, each session contains a series of product_ids\n",
    "    '''\n",
    "    action_data = action_data.sort_values(by=['user_id'],ascending=True)\n",
    "    group_action_data = action_data.groupby('user_id').agg(list)\n",
    "    session_list = group_action_data['product_ids'].to_list()\n",
    "    return session_list\n",
    "\n",
    "def get_graph_context_all_pairs(walks,window_size):\n",
    "    all_pairs = []\n",
    "    for k in range(len(walks)):\n",
    "        for i in range(len(walks[k])):\n",
    "            for j in range(i-window_size,i+window_size+1):\n",
    "                if i==j or j<0 or j>=len(walks[k]):\n",
    "                    continue\n",
    "                else:\n",
    "                    all_pairs.append([walks[k][i],walks[k][j]])\n",
    "    return np.array(all_pairs,dtype=np.int32)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(p=0.25,q=2,num_walks=10,walk_length=10,window_size=5):\n",
    "    \n",
    "    # STEP 1: get user sessions from user history file\n",
    "\n",
    "    action_data = process_nevo_data()\n",
    "    all_skus = action_data['product_ids'].unique()\n",
    "    all_skus = pd.DataFrame({'product_ids':list(all_skus)})\n",
    "    sku_lbe = LabelEncoder()\n",
    "    all_skus['product_ids'] = sku_lbe.fit_transform(all_skus['product_ids'])\n",
    "    action_data['product_ids'] = sku_lbe.transform(action_data['product_ids'])\n",
    "    \n",
    "    # collect sessions from all users\n",
    "    session_list = get_session(action_data)\n",
    "    session_list_all = []\n",
    "    for item_list in session_list:\n",
    "        if len(item_list) > 1: # single node is non-informative\n",
    "            session_list_all.append(item_list)\n",
    "    \n",
    "    # session to (directed and weighted) graph, save temp results\n",
    "    node_pair = dict()\n",
    "    for session in session_list_all:\n",
    "        for i in range(1, len(session)):\n",
    "            if (session[i-1],session[i]) not in node_pair.keys():\n",
    "                node_pair[(session[i-1], session[i])] = 1\n",
    "            else:\n",
    "                node_pair[(session[i-1], session[i])] += 1\n",
    "                \n",
    "    in_node_list = list(map(lambda x:x[0], list(node_pair.keys()))) # in node -> out node\n",
    "    out_node_list = list(map(lambda x:x[1], list(node_pair.keys())))\n",
    "    weight_list = list(node_pair.values())\n",
    "    graph_df = pd.DataFrame({'in_node': in_node_list,'out_node':out_node_list,'weight':weight_list})\n",
    "    graph_df.to_csv(\"./nevodata_cache/graph.csv\",sep=\" \",index=False,header=False)\n",
    "    \n",
    "    G = nx.read_edgelist(\"./nevodata_cache/graph.csv\", \n",
    "                         create_using=nx.DiGraph(), \n",
    "                         nodetype=None, \n",
    "                         data=[('weight',int)])\n",
    "    walker = RandomWalker(G, p=p, q=q)\n",
    "    walker.preprocess_transition_probs()\n",
    "    \n",
    "    session_reproduce = walker.simulate_walks(num_walks=num_walks,walk_length=walk_length,workers=4,verbose=1)\n",
    "    session_reproduce = list(filter(lambda x: len(x)>2, session_reproduce))\n",
    "\n",
    "    # STEP 2: get side information from product attribute file\n",
    "    product_data = pd.read_csv(\"./nevodata/product_data.csv\")\n",
    "    product = product_data[['product_ids', 'sugar', 'mushy']].copy() # ignore price for now\n",
    "    product.drop_duplicates(inplace=True)\n",
    "    all_skus['product_ids'] = sku_lbe.inverse_transform(all_skus['product_ids'])\n",
    "    sku_side_info = pd.merge(all_skus, product, on='product_ids',how='left').fillna(0)\n",
    "    \n",
    "    for product_attr in sku_side_info.columns:\n",
    "        if product_attr != 'product_ids':\n",
    "            lbe = LabelEncoder()\n",
    "            sku_side_info[product_attr] = lbe.fit_transform(sku_side_info[product_attr])\n",
    "        else:\n",
    "            sku_side_info[product_attr] = sku_lbe.transform(sku_side_info[product_attr])\n",
    "            \n",
    "    sku_side_info = sku_side_info.sort_values(by=['product_ids'],ascending=True)\n",
    "    sku_side_info.to_csv(\"./nevodata_cache/sku_side_info.csv\",index=False,header=False,sep=\" \")\n",
    "    \n",
    "    # get pair\n",
    "    all_pairs = get_graph_context_all_pairs(session_reproduce,window_size)\n",
    "    np.savetxt(\"./nevodata_cache/all_pairs.txt\",X=all_pairs,fmt='%d',delimiter=\" \")\n",
    " \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "data_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Graph Embedding with Side information (EGES) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGES_Model:\n",
    "    def __init__(self, num_nodes, num_attr, attr_lens, n_sampled=100, embedding_dim=128, lr=0.001):\n",
    "        '''\n",
    "        num_attr: number of product attributes + 1, which is the product itself (as one embedding)\n",
    "        attr_lens: a list of attribute lengths. Each attribute is represented by a one-hot vector, \n",
    "            the dimension/length depends on how many unique values are there for this attribute.\n",
    "        '''\n",
    "        self.n_sampled = n_sampled\n",
    "        self.num_attr = num_attr\n",
    "        self.attr_lens = attr_lens\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_nodes = num_nodes\n",
    "        self.lr = lr\n",
    "        self.softmax_w = tf.Variable(tf.truncated_normal((num_nodes,embedding_dim),stddev=0.1),name='softmax_w')\n",
    "        self.softmax_b = tf.Variable(tf.zeros(num_nodes),name='softmax_b')\n",
    "        self.inputs = self.input_init()\n",
    "        self.embedding = self.embedding_init()\n",
    "        self.alpha_embedding = tf.Variable(tf.random_uniform((num_nodes,num_attr),-1,1))\n",
    "        self.merge_emb = self.attention_merge()\n",
    "        self.cost = self.make_skipgram_loss()\n",
    "        self.train_op = tf.train.AdamOptimizer(lr).minimize(self.cost)\n",
    "        \n",
    "    def embedding_init(self):\n",
    "        '''\n",
    "        each product attribute has its own embedding\n",
    "        '''\n",
    "        embedding_collection = []\n",
    "        for i in range(self.num_attr):\n",
    "            embedding_var = tf.Variable(tf.random_uniform((self.attr_lens[i],self.embedding_dim),-1,1), name='embedding'+str(i),trainable=True)\n",
    "            embedding_collection.append(embedding_var)\n",
    "        return embedding_collection\n",
    "    \n",
    "    def attention_merge(self):\n",
    "        embed_list = []\n",
    "        num_embed_list = []\n",
    "        for i in range(self.num_attr):\n",
    "            cat_embed = tf.nn.embedding_lookup(self.embedding[i],self.inputs[i])\n",
    "            embed_list.append(cat_embed)\n",
    "        stack_embed = tf.stack(embed_list, axis=-1)\n",
    "        # attention merge\n",
    "        alpha_embed = tf.nn.embedding_lookup(self.alpha_embedding, self.inputs[0])\n",
    "        alpha_embed_expand = tf.expand_dims(alpha_embed, 1)\n",
    "        merge_emb = tf.reduce_sum(stack_embed * tf.exp(alpha_embed_expand),axis=-1) / tf.reduce_sum(tf.exp(alpha_embed_expand),axis=-1)\n",
    "        return merge_emb\n",
    "    \n",
    "    def input_init(self):\n",
    "        '''\n",
    "        initialize attribute inputs, label stored in the last element\n",
    "        '''\n",
    "        input_list = []\n",
    "        for i in range(self.num_attr):\n",
    "            input_col = tf.placeholder(tf.int32,[None],name='inputs_'+str(i))\n",
    "            input_list.append(input_col)\n",
    "        input_list.append(tf.placeholder(tf.int32,shape=[None,1],name='label'))\n",
    "        return input_list\n",
    "    \n",
    "    def make_skipgram_loss(self):\n",
    "        '''\n",
    "        sampled softmax loss: faster when there are huge number of classes, for training purpose only\n",
    "        '''\n",
    "        loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "            weights=self.softmax_w,\n",
    "            biases=self.softmax_b,\n",
    "            labels=self.inputs[-1],\n",
    "            inputs=self.merge_emb,\n",
    "            num_sampled=self.n_sampled,\n",
    "            num_classes=self.num_nodes,\n",
    "            num_true=1,\n",
    "            sampled_values=tf.random.uniform_candidate_sampler(\n",
    "                true_classes=tf.cast(self.inputs[-1],tf.int64),\n",
    "                num_true=1,\n",
    "                num_sampled=self.n_sampled,\n",
    "                unique=True,\n",
    "                range_max=self.num_nodes\n",
    "            )\n",
    "        ))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and get the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EGES(output_path,batch_size=2048,n_sampled=10,epochs=2,lr=0.001,num_attr=3,embedding_dim=128,if_plot=False):\n",
    "    \n",
    "    side_info = np.loadtxt(\"./nevodata_cache/sku_side_info.csv\",dtype=np.int32,delimiter=\" \")\n",
    "    all_pairs = np.loadtxt(\"./nevodata_cache/all_pairs.txt\",dtype=np.int32,delimiter=\" \")\n",
    "    attr_lens = []\n",
    "    for i in range(side_info.shape[1]):\n",
    "        tmp_len = len(set(side_info[:,i]))\n",
    "        attr_lens.append(tmp_len)\n",
    "    \n",
    "    num_nodes = len(side_info)\n",
    "    EGES = EGES_Model(num_nodes,num_attr,attr_lens,n_sampled,embedding_dim,lr)\n",
    "    \n",
    "    # initialize model\n",
    "    init = tf.global_variables_initializer()\n",
    "    config_tf = tf.ConfigProto()\n",
    "    config_tf.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config_tf)\n",
    "    sess.run(init)\n",
    "    \n",
    "    print_every_k_iterations = 100\n",
    "    loss = 0\n",
    "    iteration = 0\n",
    "    \n",
    "    max_iter = len(all_pairs) // batch_size * epochs\n",
    "    for each_iter in range(max_iter):\n",
    "        iteration += 1\n",
    "        batch_features, batch_labels = next(graph_context_batch_iter(all_pairs,batch_size,side_info,num_attr))\n",
    "        feed_dict = {input_col: batch_features[:,i] for i,input_col in enumerate(EGES.inputs[:-1])}\n",
    "        feed_dict[EGES.inputs[-1]] = batch_labels\n",
    "        _, train_loss = sess.run([EGES.train_op,EGES.cost],feed_dict=feed_dict)\n",
    "        loss += train_loss\n",
    "        \n",
    "        if iteration % print_every_k_iterations == 0:\n",
    "            e = iteration * batch_size//len(all_pairs)\n",
    "            print('Epoch {}/{}'.format(e,epochs),\n",
    "                  'Iteration: {}'.format(iteration), \n",
    "                  'Average training loss: {:.4f}'.format(loss/print_every_k_iterations)\n",
    "                  )\n",
    "            loss = 0\n",
    "    print('optimization finished! saving results...')\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,\"./nevodata_cache/checkpoints/EGES\")\n",
    "    \n",
    "\n",
    "    feed_dict_test = {input_col: list(side_info[:,i]) for i,input_col in enumerate(EGES.inputs[:-1])}\n",
    "    feed_dict_test[EGES.inputs[-1]] = np.zeros((len(side_info),1),dtype=np.int32)\n",
    "    embedding_result = sess.run(EGES.merge_emb,feed_dict=feed_dict_test)\n",
    "    write_embedding(embedding_result, output_path)\n",
    "    \n",
    "    # plot the result\n",
    "    if if_plot == True:\n",
    "        print('visualization...')\n",
    "        plot_embeddings(embedding_result[:5000,:],side_info[:5000,:])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4 Iteration: 100 Average training loss: 0.6336\n",
      "Epoch 0/4 Iteration: 200 Average training loss: 0.5623\n",
      "Epoch 0/4 Iteration: 300 Average training loss: 0.5242\n",
      "Epoch 0/4 Iteration: 400 Average training loss: 0.5078\n",
      "Epoch 0/4 Iteration: 500 Average training loss: 0.4678\n",
      "Epoch 0/4 Iteration: 600 Average training loss: 0.4512\n",
      "Epoch 1/4 Iteration: 700 Average training loss: 0.4013\n",
      "Epoch 1/4 Iteration: 800 Average training loss: 0.3782\n",
      "Epoch 1/4 Iteration: 900 Average training loss: 0.3874\n",
      "Epoch 1/4 Iteration: 1000 Average training loss: 0.3527\n",
      "Epoch 1/4 Iteration: 1100 Average training loss: 0.3547\n",
      "Epoch 1/4 Iteration: 1200 Average training loss: 0.3125\n",
      "Epoch 1/4 Iteration: 1300 Average training loss: 0.3088\n",
      "Epoch 2/4 Iteration: 1400 Average training loss: 0.3030\n",
      "Epoch 2/4 Iteration: 1500 Average training loss: 0.2768\n",
      "Epoch 2/4 Iteration: 1600 Average training loss: 0.3047\n",
      "Epoch 2/4 Iteration: 1700 Average training loss: 0.3278\n",
      "Epoch 2/4 Iteration: 1800 Average training loss: 0.3250\n",
      "Epoch 2/4 Iteration: 1900 Average training loss: 0.3329\n",
      "Epoch 2/4 Iteration: 2000 Average training loss: 0.3028\n",
      "Epoch 3/4 Iteration: 2100 Average training loss: 0.3461\n",
      "Epoch 3/4 Iteration: 2200 Average training loss: 0.3305\n",
      "Epoch 3/4 Iteration: 2300 Average training loss: 0.3548\n",
      "Epoch 3/4 Iteration: 2400 Average training loss: 0.3162\n",
      "Epoch 3/4 Iteration: 2500 Average training loss: 0.3443\n",
      "Epoch 3/4 Iteration: 2600 Average training loss: 0.3163\n",
      "Epoch 3/4 Iteration: 2700 Average training loss: 0.3430\n",
      "Epoch 4/4 Iteration: 2800 Average training loss: 0.2390\n",
      "optimization finished! saving results...\n"
     ]
    }
   ],
   "source": [
    "out = \"./nevodata_cache/eges.embed\"\n",
    "run_EGES(out,batch_size=16,n_sampled=1,epochs=4,embedding_dim=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
